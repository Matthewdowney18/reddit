from nltk import word_tokenize
import string

'''
this file takes the csv of the reddit dataset, and it prepares it for training.
first it strips the text of urls numbers, and then it takes out puncuation, 
lower cases the words, and then it tokenizes, and joins the tokens with 
spaces to make it easy to do things ye dawg 
'''

def main:



if __name__ == '__main__':
    main()